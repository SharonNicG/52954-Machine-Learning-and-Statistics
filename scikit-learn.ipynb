{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "982054bc-1e99-479f-8f9a-b35d0a280554",
   "metadata": {},
   "source": [
    "# Machine Learning and Statistics\n",
    "***\n",
    "\n",
    "# scikit-learn\n",
    "***\n",
    "\n",
    "![image](https://github.com/SharonNicG/52954-Machine-Learning-and-Statistics/blob/main/Images/640px-Scikit_learn_logo_small.svg.jpg)\n",
    "\n",
    "<br>\n",
    "\n",
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900a3512-0aa9-4296-9ef8-112888d382ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Evenly spaces values in arrays\n",
    "from numpy import arange\n",
    "# Data frames\n",
    "import pandas as pd\n",
    "# Read CSV data quickly into Pandas\n",
    "from pandas import read_csv\n",
    "# Customise display-related options\n",
    "from pandas import set_option\n",
    "# Explore trends in data\n",
    "# https://www.marsja.se/pandas-scatter-matrix-pair-plot/\n",
    "from pandas.plotting import scatter_matrix\n",
    "# pyplot is a module that is a simple and easy way to construct plots\n",
    "from matplotlib import pyplot\n",
    "# Allows matplotlib graphs to be included in the notebook next to the code\n",
    "%matplotlib inline\n",
    "# Fancier, statistical plots\n",
    "import seaborn as sns\n",
    "# Measures classification performance\n",
    "from sklearn import metrics\n",
    "# Model fit metrics\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "# Data transformations\n",
    "from sklearn import preprocessing\n",
    "# Standardization, mean removal and variance scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Evaluation scores by cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# Cross-validation iterator\n",
    "from sklearn.model_selection import KFold\n",
    "# Splits arrays or matrices into random train and test subsets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Combines preprocessor and the classifier into a single pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Functionaility for neighbors-based machine learning\n",
    "from sklearn import neighbors\n",
    "# K-Nearest Neighbours Regression functionaility \n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "# Linear regression functionaility\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Decision Tree Regression functionality\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# Plotting for Decision Tree algorithm\n",
    "from sklearn.tree import plot_tree\n",
    "# Returns the square root of any number\n",
    "from math import sqrt\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cece74-b416-46f3-8bb5-292a9ae6febd",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Statistics is the study of data. It identifies data's properties and/or the assumptions\n",
    "made about data built on the information provided [1]. \n",
    "\n",
    "In machine learning, statistical methods are used to solve defined problems. The 'machine' is programmed to apply algorithms to fit an analysis model to the data provided. In fitting the model to the data, the 'machine' learns how to treat variables in the dataset. It uses that knowledge to improve the accuracy of its results each time[2]. \n",
    "\n",
    "Providing understandable and workable data is vital to machine learning. The 'machine' is programmed to execute a series of actions on the provided dataset, not to determine if it is correct. For example, applying machine learning to a dataset that describes features of wine [3] and their associated quality score index could help predict the quality of wines outside of the dataset. For example, if all known Barolo wines produced in 2010 have a high-quality score, and the 'machine' is presented with details of a new 2010 Barolo wine, it is likely to predict that the wine is good. However, if the wine were actually 2017 Barolo, and the wrong data was supplied, the score assigned would be inaccurate because of the inputted data and not the Machine Learning process.\n",
    "\n",
    "***\n",
    "\n",
    "# scikit-learn \n",
    "\n",
    "scikit-learn [4] is a free, open-source library of machine learning tools compatible with Python[5]. \n",
    "\n",
    "It was developed in 2007 as a Google Summer of Code project [6] in 2007, by data scientist David Cournapeau. The name scikit describes the idea that the library is a toolkit for the SciPy library – a Scikit. The project has grown significantly since and was first released to the public in 2010 [7].\n",
    "\n",
    "The library consists of ready to use machine learning algorithms that can be applied to a problem using the Python interface. This includes algorithms for classification, regression, clustering, dimensionality reduction, model fitting, and pre-processing [8][9].\n",
    "\n",
    "As an open-source product, it encourages innovation as the library evolves through application by its users. Working within the Python ecosystem means that scikit-learn is available to a fast-growing community of users [10], can work with the other libraries within Python – including SciPy [11], which it is built upon – and has the flexibility to work with different programming styles and on different platforms [12]. \n",
    "\n",
    "Using scikit-learn provides a level of robustness to machine learning in Python. Furthermore, using its prebuilt algorithms offers reliability and efficiency challenging to achieve in 'hardcoding' an algorithm. \n",
    "\n",
    "It also makes the complicated mathematical functions used in machine learning algorithms more accessible to programmers. Of course, understanding the fundamentals of statistics and modelling is necessary to use the scikit-learn, but the library provides abstraction from the core mathematics for the user.  \n",
    "\n",
    "***\n",
    "\n",
    "## Machine Learning Styles\n",
    "There are several ways to conduct machine learning. First, the machine learning style used to create an algorithm is selected based on the types of input and outputs for the problem.\n",
    "The four main learning styles are:\n",
    "\n",
    "- Supervised \n",
    "- Unsupervised \n",
    "- Autonomous/Self-Supervised \n",
    "- Reinforcement \n",
    "\n",
    "**Supervised**\n",
    "\n",
    "Supervised machine learning takes data that has been labelled (tagged or classed with an identifier) as its input and returns an expected result as its output[13][14]. \n",
    "\n",
    "In Supervised machine learning, algorithms use training to create a model. An algorithm is then applied to fit that model to the data provided. The 'machine' learns through continuous training, and results become more accurate.\n",
    "\n",
    "The expected results of training help determine which algorithm to apply. Regression problems seek to predict a numeric/outcome—for example, the quality score of a wine based on the grape type and year of production. Classification problems seek a qualitative variable/result. For example, determining a flower's class based on their sepal and petal measurements. Supervised Learning algorithms (like Linear Regression [15] and K-Nearest Neighbour [16] are bested suited to these problems. \n",
    "\n",
    "**Unsupervised**\n",
    "\n",
    "In unsupervised machine learning, the data isn't labelled, and the results are unknown [17] [18]. Here the data is analysed to identify any structures therein. This analysis produces a model for analysing the data. The 'machine' learns how to fit the model to the data based on its analysis of the information provided. The algorithms applied to these problems can cluster similar data together or detect anomalies in the dataset.\n",
    "\n",
    "Unsupervised learning algorithms (like Factor Analysis [19] and Outlier Detection [20] are bested suited to these problems. \n",
    "\n",
    "**Autonomous/Self-Supervised**\n",
    "\n",
    "Autonomous/self-supervised machine learning is like supervised machine learning as it takes labelled data and returns an expected output [21]. However, it doesn't require the result to be defined. Instead, it learns the output's label based on externally sources or data patterns. \n",
    "The sklearn.semi_supervised module [22] and scikit-learn algorithms for regression and classification work can adapt models to act like an autonomous/semi-supervised algorithm [23].\n",
    "\n",
    "**Reinforcement**\n",
    "\n",
    "Reinforcement machine learning works similarly to Autonomous/Self-supervised machine learning [24]. But with the addition of reinforcement of a program's behaviour based on positive or negative feedback. The requirements of this type of machine learning are out of the scope of the scikit-learn library [25].\n",
    "\n",
    "***\n",
    "\n",
    "This notebook will focus on Supervised Machine Learning using three different algorithms. Three different algorithms will be applied to the same dataset to see the different ways machine learning can analyse a problem and the variation in outputs they can provide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789bf2ed-24c2-4e84-98b1-9c54a2341e41",
   "metadata": {},
   "source": [
    "### Boston House Price Dataset\n",
    "\n",
    "![image](https://github.com/SharonNicG/52954-Machine-Learning-and-Statistics/blob/main/Images/michael-browning-ZLN2WOVpjCo-unsplash.jpg)\n",
    "\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0a8c7e-9497-447d-b16c-5f50d095015e",
   "metadata": {},
   "source": [
    "The Boston House Price dataset contains information about houses in a Boston suburb or town drawn from the Boston Standard Metropolitan Statistical Area (SMSA) in 1970 [26].\n",
    "\n",
    "This data was previously a part of the UCI Machine Learning Repository [27] and was inbuilt to the scikit-learn library. However, it has since been removed from the UCI Machine Learning Repository and is being removed from the scikit-learn library. This is due to ethical issues around assumptions made in creating the dataset and the validity of its purpose [28]. More information on this is available here on the [scikit-learn website](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html).\n",
    "\n",
    "An essential lesson for anyone working with data is to trust but verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8945b6f-3291-4e0a-b79d-752e36860d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import load_boston\n",
    "# df_boston = load_boston()\n",
    "# print(df_boston.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4a0859-ed85-4747-b2ce-bd3b9044a0cf",
   "metadata": {},
   "source": [
    "To futureproof the functioning of this notebook, the dataset was downloaded and read into the notebook from a CSV file.\n",
    "\n",
    "Details of the dataset characteristics available as part of the UCI Machine Learning Repository and scikit-learn library are reproduced below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b25845-e746-4830-923c-e1af9cea7bcf",
   "metadata": {},
   "source": [
    "**Data Set Characteristics**  \n",
    "\n",
    "    Number of Instances: 506 \n",
    "\n",
    "    Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
    "\n",
    "    Attribute Information (in order):\n",
    "        - CRIM     per capita crime rate by town\n",
    "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "        - INDUS    proportion of non-retail business acres per town\n",
    "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "        - NOX      nitric oxides concentration (parts per 10 million)\n",
    "        - RM       average number of rooms per dwelling\n",
    "        - AGE      proportion of owner-occupied units built prior to 1940\n",
    "        - DIS      weighted distances to five Boston employment centres\n",
    "        - RAD      index of accessibility to radial highways\n",
    "        - TAX      full-value property-tax rate per $10,000\n",
    "        - PTRATIO  pupil-teacher ratio by town\n",
    "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "        - LSTAT    % lower status of the population\n",
    "        - MEDV     Median value of owner-occupied homes in $1000's\n",
    "\n",
    "    Missing Attribute Values: None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f685ac6-a60a-44fb-af3c-ea3d9a3946aa",
   "metadata": {},
   "source": [
    "The version downloaded from http://lib.stat.cmu.edu/datasets/boston appears to have been processed. However, online research shows that other dataset versions are missing values. No missing values are recorded in this dataset.\n",
    "\n",
    "For easier referencing late in the model, the attributes have been assigned short names matching those used by the UCI Machine Learning Repository and the scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766d6c8d-6633-46b1-8142-eaa8f4fd2905",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Data/housing.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aee48aa-d48e-4cfb-b25e-ea16d8af54ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2fb3c3-7ee5-471b-8597-293681b0df2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_csv(filename, delim_whitespace=True, names=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1edafb0-59db-4e99-a270-b4ab905ee64c",
   "metadata": {},
   "source": [
    "#### 1. Analyze Data\n",
    "\n",
    "An initial look at the loaded data [29].\n",
    "\n",
    "Confirming the dimensions of the dataset - the number of rows and columns - match the characteristics expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dba642-ede7-4ada-bb6a-7aa669eb9bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c44b627-0f9a-470f-97f3-7fa303dcf4d6",
   "metadata": {},
   "source": [
    "The results show 506 data instances with 14 attributes.\n",
    "\n",
    "Now that we know the size and shape of the data, let's look at the types of data associated with each attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b39166-96b2-4915-8ec4-607f17f59a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57a78b9-b435-446c-af06-cd6c7f8d50d6",
   "metadata": {},
   "source": [
    "All attributes are numeric, mostly floats, and two are integers.\n",
    "\n",
    "The next step is to have a quick look at the data (the first 10 rows of the dataset) to see what it looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499be634-92d4-4aa5-8084-28b3202a7c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f07bf13-d04c-4e6a-9bfe-c76231ee1d9a",
   "metadata": {},
   "source": [
    "The values associated with each attribute varies widely (e.g. RAD 1-5 and CRIM 1.05-9.38).\n",
    "\n",
    "To get a better feel for the ranges of these distributions, we can request the statistical descriptions of each attribute. Setting the precision for the data will make it easier for the floats (the primary data type)[30]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497d2710-61dd-45c6-89d2-9b6bae584ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_option('precision', 2)\n",
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c89d530-6971-4b0f-ba73-765147b8d941",
   "metadata": {},
   "source": [
    "Again there is a wide variation staticial values ofr each attribute (e.g. CRIM min = 6.32 amd TAX min = 187.00).\n",
    "\n",
    "Finally, we can look at the correlation between the attribute columns for this initial analysis.\n",
    "The Pearson method is used here. This is the standard correlation coefficient - it measures the strength of a linear association between two variables [31]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7185001-e043-4f5d-b520-44da40246f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_option('precision', 2)\n",
    "print(dataset.corr(method='pearson'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc97cf00-8c4a-431f-aef0-f96c320d8dc8",
   "metadata": {},
   "source": [
    "Some of the attributes appear to have a strong correlation. For example, \n",
    "\n",
    "- AGE and NOX with 0.73\n",
    "- DIS and NOX with -0.78\n",
    "- DIS and INDUS with -0.71"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8834bf88-1cf3-438c-b1b2-e9e01b4377a6",
   "metadata": {},
   "source": [
    "#### 2. Data Visualisation\n",
    "\n",
    "While the statistical information is informative, data visualisations can help identify trends and patterns [32].\n",
    "\n",
    "Histograms are useful for viewing the distribution of data in a dataset. Let's look at histograms for each of the attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cab373a-53ac-4461-b4c6-f71d2a125e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.hist(figsize=(20, 10))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e0c701-3fd1-419f-983a-94f53f251a27",
   "metadata": {},
   "source": [
    "The histograms show some attributes such as CRIM and AGE have an exponential distribution [33]. Others, such as RAD and TAX, demonstrate data clusters with peaks at either end of their scale.\n",
    "\n",
    "Plotting the same data using Density Plots [34] allows us to see the same data represented in the histograms but as a smoothed-out continuous line. This is helpful when looking at several attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18431404-b203-4896-a404-14863be389e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.plot(kind='density', subplots=True, layout=(4,4), sharex=False)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf5a49c-5416-4030-99da-b9776f073d13",
   "metadata": {},
   "source": [
    "The distribution patterns seen in the histograms are much easier to see here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0126eb3-f49e-4926-854e-237209adf0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.plot(kind='density', subplots=True, layout=(4,4), sharex=False,figsize=(20, 10))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbf9813-b020-4e20-a7f6-adba3700b91a",
   "metadata": {},
   "source": [
    "Using Box and Whisker plots, it is possible to see how the data is spread within each attribute [35]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40991f72-f362-4f44-8d73-4d89e7ece624",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.plot(kind='box', subplots=True, layout=(4,4), sharex=False, sharey=False,figsize=(20, 10))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fabd16-026c-451d-9918-a8ad6db595c9",
   "metadata": {},
   "source": [
    "Having looked at the distribution and spread of data within each attribute, let's look at how the attributes interact. A Scatter Matrix plots all the numeric variables in a dataset against each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801da348-bc7a-496b-a2b6-3d973ae8209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(dataset,figsize=(20, 10))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703823ae-146d-4313-93b8-7ce5b5fd2735",
   "metadata": {},
   "source": [
    "The Scatter Matrix shows that the attributes identified as having a strong correlation have a clear relationship. But it is a little hard to view given the number of variables in the dataset. \n",
    "\n",
    "Let's look at the same information in pairplots to understand the relationships better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d62f00-39d7-410c-b8f8-a96363b74c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairplot with hue day\n",
    "for i in dataset.columns:\n",
    "  pyplot.figure(figsize=(8,6))\n",
    "  sns.scatterplot(dataset[i],dataset['MEDV'])\n",
    "  pyplot.xlabel(i)\n",
    "  pyplot.ylabel('MEDV')\n",
    "  pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85f1349-8567-4468-b2df-03c655624146",
   "metadata": {},
   "source": [
    "We can investigate these relationships further by building a correlation matrix [36] [37]. This is a table showing correlation coefficients between the attributes. Each cell shows the correlation between two attributes. The values for this correlation is between -1 and 1 (vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7b4990-ab95-43f3-976c-b3e1919ea3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pyplot.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "cax = ax.matshow(dataset.corr(), vmin=-1, vmax=1)\n",
    "\n",
    "fig.colorbar(cax)\n",
    "\n",
    "ticks = arange(0,14,1)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(names)\n",
    "ax.set_yticklabels(names)\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7d82cf-6ca1-453e-af93-d33df8c354ba",
   "metadata": {},
   "source": [
    "The yellow and lighter coloured cells show a positive correlation between attributes.\n",
    "In contrast, the dark blue and other darker cells negatively correlate between attributes.\n",
    "\n",
    "This process is repeated below, with the correlation value printed inside the cells to help identify outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d1b238-3bfb-4461-b942-3834a8ed4548",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = dataset.corr().round(2)\n",
    "pyplot.figure(figsize=(14,10))\n",
    "sns.heatmap(data=correlation_matrix,annot=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ab9a30-4671-4875-8372-80e35726ae0b",
   "metadata": {},
   "source": [
    "#### 3. Preprocessing\n",
    "\n",
    "To check the model's accuracy, the dataset is split into two parts. The first is used to train the 'machine' and develop the model. This is usually the larger part in a two-way split on smaller and less complex datasets such as this one. The remainder of the data will be used to validate the model's performance (using algorithms from scikit-learn) once trained. By running a portion of unseen data through the trained model, it is possible to see if it is working and how well. The `sklearn.model_selection.train_test_split`[38] function supports this stage.\n",
    "\n",
    "There is much debate over the optimum split in data for this stage. Generally, where a dataset is small or less complex, the 70:30 (training: validation) approach taken here is used. However, for more complex datasets (multi-dimensional) or a large volume of data, a heavier weighting toward training is advised to strengthen the machine learning process [39]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc7933e-ef7e-471d-8fc6-3e5bf5593f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate out a validation set from the training set (test_size)\n",
    "# have toke the 'MEDV' as dependent variable store in Y and all others as independent vatiables store in X for our models\n",
    "array = dataset.values\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "\n",
    "# Train size 70%\n",
    "# Test size 30%\n",
    "test_size = 0.30\n",
    "\n",
    "# Random state is 42\n",
    "seed = 42\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b0d0e7-3a04-4aad-9637-418b1870656f",
   "metadata": {},
   "source": [
    "Alternatively, the dataset can be split three ways: training, validation and testing. The testing portion is used to test the general performance using data that has been unseen during the training and validation stages [40]. \n",
    "\n",
    "The scikit-learn library includes functions to complete k-fold cross-validation for evaluating the model's performance [41]. A 10-fold validation is applied here [42]. This splits the dataset into ten parts (folds). All bar one parts are trained, and the reserved part is then tested. This process repeats until all parts have been the test part. The result evaluates the model's performance based on ten cross-checks. \n",
    "\n",
    "A random state seed [43] is applied [44] to the data splitting to make the work reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667db2d0-6574-4fd6-a331-2fba3a034a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The K_Fold cross validation will validate algorithm 10 time\n",
    "num_folds = 10\n",
    "\n",
    "# Random state for K_Fold Cross Validation\n",
    "seed = 7\n",
    "\n",
    "# Using Mean Squared Error to gauge how many predictions are wrong\n",
    "# Model-evaluation tools using cross-validation (such as model_selection.cross_val_score and model_selection.GridSearchCV) rely on an internal scoring strategy.\n",
    "scoring = 'neg_mean_squared_error'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a34769-fcad-4cd6-8d18-5dfccb719e75",
   "metadata": {},
   "source": [
    "Next, we take the three algorithms selected for exploration.  \n",
    "\n",
    "**Linear Regression (LR)**\n",
    "\n",
    "Linear regression is a supervised machine learning model that looks at the relationship between variables [45]. There is an independent variable and a dependent variable in simple linear regression. The independent variable is the input value of the model. The predicted output for those values is the dependant variable.\n",
    "\n",
    "Linear regression models look at the statistical relationship between these variables. It is not deterministic [46]. The input cannot entirely predict the output. But the relationship between the variables is close and continuous. \n",
    "\n",
    "Linear regression aims to find a line that best fits the data. That is a predicted line (line of regression) that sits close to the data (data points). Prediction errors create distance between the data and the line and reduce the model's accuracy [47]. \n",
    "\n",
    "![image](https://github.com/SharonNicG/52954-Machine-Learning-and-Statistics/blob/main/Images/analyticsvidhya_linearregression.png) \n",
    "\n",
    "Multiple Linear Regressions function similarly but have two or more independent variables.  \n",
    "\n",
    "<br>\n",
    "\n",
    "**K-Nearest Neighbours (KNN)**\n",
    "\n",
    "K Nearest Neighbours is a non-linear supervised learning algorithm. It uses distance measures to find the most similar data to the one presented to predict its outcome [48]. \n",
    "\n",
    "It does this by plotting the data provided, looking at an identified datapoint and calculating the distance between it and the other data points. The closest data points to the one being examined (the nearest neighbours) are predicted to have an association [49]. Repeating this process for all data points in the training set provides the model with a series of outcomes. Averaging the results of these outcomes indicate the statistical relationship between the data points (variables) [50][51].\n",
    "Setting the value of K determines how many data points are considered when reviewing the relationship between datapoints—for example, basing the connection on the two nearest neighbours to a data point or the seven nearest neighbours. \n",
    "\n",
    "![image](https://github.com/SharonNicG/52954-Machine-Learning-and-Statistics/blob/main/Images/datacamp_KNN.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "**Decision Tree Regression (CART)**\n",
    "\n",
    "Decision Tree Regression is a supervised machine learning algorithm that models decisions through a tree-like structure – similar to a flow chart [52]. It is referred to as CART, which stands for Classification and Regression Tree algorithm.\n",
    "\n",
    "It provides a graphical representation of the decision-making process, which may be more accessible than statistical information [53].\n",
    "\n",
    "Starting from the Root Node (the dataset), branches flow to Interior Nodes where True/False decisions are made based on the features of the dataset [54]. This is repeated until an outcome is reached at the Leaf Node level. Finally, the average value of all data points in a Leaf Node is used to predict the probability of an event/output occurring. \n",
    "\n",
    "\n",
    "![image](https://github.com/SharonNicG/52954-Machine-Learning-and-Statistics/blob/main/Images/decision-tree-classification-algorithm.png)\n",
    "\n",
    "The algorithm can be used for classification and regression modelling with the scikit-learn library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb40ffb-fa89-4c18-8c8e-f47f556314f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing three models in the list\n",
    "models = []\n",
    "models.append(('LR', LinearRegression()))\n",
    "models.append(('KNN', KNeighborsRegressor()))\n",
    "models.append(('CART', DecisionTreeRegressor()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf4b570-b335-4b94-8b8f-390e9667ebb0",
   "metadata": {},
   "source": [
    "Each will complete an initial test of mean and standard deviation of Mean Squared Error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de01013-4a1d-411a-8276-d5d274721d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "result = []\n",
    "names = []\n",
    "\n",
    "# Check the result of each of three models\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=num_folds, random_state=42, shuffle=True)\n",
    "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    result.append(cv_results.mean())\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3455b322-a3e6-4a38-b0a4-92e73747fba0",
   "metadata": {},
   "source": [
    "Linear Regression has the lowest Mean Squared Error (0 being the perfect score). The Decision Tree Regression is next, with scores close to Linear Regressions. K-Nearest neighbour is significantly different. \n",
    "\n",
    "We can visualise the scores from across all cross-validated folds to see the distribution of scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf26d63-ba60-4584-b94c-7350b364afdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b301bf83-6104-4eb4-920f-f118e1800a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pyplot.figure(figsize=(12,8))\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f492ac-ea16-4ae4-a61b-7d7b361cd6aa",
   "metadata": {},
   "source": [
    "The distributions appear to be similar even for the K-Nearest neighbour.\n",
    "\n",
    "This box plot shows the range of scoring for each model. In Linear Regression, the values of the upper and lower ranges are very close to each other. While in the other, these are a bit away from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85988c57-7ba6-41e8-afcc-374946e5089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pyplot.figure(figsize=(12,8))\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "sns.set_theme(style=\"white\", context=\"talk\")\n",
    "sns.barplot(names,result,palette=\"rocket\")\n",
    "#ax.set_xticklabels(names)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aa7ad6-a1a2-4d33-9b63-c89458190202",
   "metadata": {},
   "source": [
    "**Standardisation and Scaling**\n",
    "\n",
    "Standardisation and scaling translate attribute values to more comparable scales [55] [56]. This refers back to the importance of the data fed into a model. The 'machine' will learn and predict based on the information provided. For example, comparing a jewellery store dataset with stock values from 0 to 100 and gold carat values from 1 to 5 may give more weight to the 100 stock units. The standard deviation and mean values for input data are brought closer to a normal distribution through standardising and scaling, making them much more comparable.\n",
    "\n",
    "The scikit-learn functions to transform [57] data in this way are in the `sklearn.preprocessing` library.\n",
    "\n",
    "Here we will transform the data using the standardisation functions and rerun the test of mean and standard deviation of Mean Squared Error for each algorithm.\n",
    "\n",
    "Pipeline sare used to automate the workflow. Pipelines operate by enabling a sequence of data to be transformed and correlated together in a model that can be tested and evaluated to achieve an outcome, whether positive or negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35cf173-0477-4b4b-9b57-d569fa9a6fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp array\n",
    "pipelines = []\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "pipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR', LinearRegression())])))\n",
    "pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsRegressor())])))\n",
    "pipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeRegressor())])))\n",
    "\n",
    "# temp arrays\n",
    "results = []\n",
    "result = []\n",
    "names = []\n",
    "for name, model in pipelines:\n",
    "    \n",
    "    kfold = KFold(n_splits=num_folds, random_state=42, shuffle=True)\n",
    "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "    \n",
    "    results.append(cv_results)\n",
    "    results.append(cv_results)\n",
    "    names.append(names)\n",
    "    \n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab60c4a1-964c-4cf1-92ec-fef406ed0df0",
   "metadata": {},
   "source": [
    "We can see that standardising and scaling the data has brought the K-Nearest Neighbours regression in line with the other results - in fact; it now returns the best score. \n",
    "\n",
    "Let's see how the distribution of scores looks following the data transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f25d1cc-d02c-4d89-b588-65af399bf4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pyplot.figure(figsize=(12,8))\n",
    "fig.suptitle('Algorithm Comparison - After Standard Scaling')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf78fef-a863-4291-b122-ec5db2b89294",
   "metadata": {},
   "source": [
    "All three algorithms are now showing a neater distribution of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63ea30f-98a0-405b-aaa3-0ddabeee7736",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pyplot.figure(figsize=(12,8))\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "sns.set_theme(style=\"white\", context=\"talk\")\n",
    "sns.barplot(names,result,palette=\"rocket\")\n",
    "#ax.set_xticklabels(names)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf126865-745c-4e6c-b436-188585fe5f68",
   "metadata": {},
   "source": [
    "#### 4. Approach 1\n",
    "\n",
    "Here each algorithm is applied against the now stndardised data. The approached used is minimal scripting with the maxium efficiency of the scikit-learn library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c457b1-8956-47a2-bf9e-6b29a00909a3",
   "metadata": {},
   "source": [
    "**Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d5692e-8e9b-4993-a095-671e7f4c06c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "LModel = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3117cc1a-cb79-4950-93a8-5cf16eed4db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model on Training Data\n",
    "LRModel = LModel.fit(X_train, Y_train)\n",
    "prediction = LRModel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b56e1b-791e-4106-a3f0-db6507d6fc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measuring Goodness of fit in Training data\n",
    "print('R2 Value:',metrics.r2_score(Y_train, LRModel.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742f3562-922d-4527-b46b-b53d100b6257",
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy_Values=cross_val_score(LRModel, X , Y, cv=10, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3745a3f1-7e3a-4890-86f1-7cd4baa06dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nAccuracy values for 10-fold Cross Validation:\\n',Accuracy_Values)\n",
    "print('\\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1f9207-2511-41b0-bc1a-604c9548789f",
   "metadata": {},
   "source": [
    "**KNN Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7531f2-75d6-4e82-9c53-643334e89ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "KModel = KNeighborsRegressor(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb78216b-a76e-4a10-9ff4-fe19525af833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model on Training Data\n",
    "KNN = KModel.fit(X_train, Y_train)\n",
    "prediction = KNN.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3e06fd-1517-4a16-a5ed-643255e5f461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measuring Goodness of fit in Training data\n",
    "print('R2 Value:',metrics.r2_score(Y_train, KNN.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23656124-1b79-461b-a6d4-79b4a6993fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy_Values=cross_val_score(KModel, X , Y, cv=10, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6366f022-9f5c-41a3-8a90-21ef5d2f84dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nAccuracy values for 10-fold Cross Validation:\\n',Accuracy_Values)\n",
    "print('\\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb67e37-b14c-4a04-9449-d8ac726f8433",
   "metadata": {},
   "source": [
    "**Decision Tree Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3001d9b1-09b3-4761-910c-3bd1886e17c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DTModel = DecisionTreeRegressor(max_depth=5,criterion='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9a2c6d-26d1-4191-88ee-811cebcda550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model on Training Data\n",
    "DT = DTModel.fit(X_train, Y_train)\n",
    "prediction=DT.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b128359a-249c-4a27-ab91-a0270c0d135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('R2 Value:',metrics.r2_score(Y_train, DT.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c13b8c5-c7a1-4c49-8b57-4c9d644dc0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy_Values=cross_val_score(DTModel, X , Y, cv=10, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db202e6c-b674-40d3-b5e3-08b040367186",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nAccuracy values for 10-fold Cross Validation:\\n',Accuracy_Values)\n",
    "print('\\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d50fe9-5995-48b1-a6e0-6c0632b1ad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure(figsize=(25,10))\n",
    "a = plot_tree(DTModel, \n",
    "              feature_names=dataset.drop('MEDV',axis=1).columns, \n",
    "              class_names=Y, \n",
    "              filled=True, \n",
    "              rounded=True, \n",
    "              fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c93d75-5ac5-4861-bd2e-000511dff83f",
   "metadata": {},
   "source": [
    "#### 5. Approach 2\n",
    "\n",
    "Here the dataset is reloaded and each regression algorithm applied in a procedural approach. \n",
    "\n",
    "\n",
    "**Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac07364f-252f-4ccb-8281-f68fbfbfb04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we take ['LSTAT','RM'] as independent variables while 'MEDV' as dependent variable for our models\n",
    "X = pd.DataFrame(np.c_[dataset['LSTAT'], dataset['RM']], columns = ['LSTAT','RM'])\n",
    "Y = dataset['MEDV']\n",
    "\n",
    "# Train size 70%\n",
    "# Test size 30%\n",
    "test_size = 0.30\n",
    "\n",
    "# Random state is 42\n",
    "seed = 42\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "lin_model = LinearRegression()\n",
    "lin_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6ef1f4-0ff4-42b2-a4d0-55b73c0655e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the Least Squares Line\n",
    "sns.pairplot(dataset, x_vars=['LSTAT','RM'], y_vars='MEDV', size=7, aspect=0.7, kind='reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dea06ca-2a67-435c-af1f-e6284f775335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measuring Goodness of fit in Training data\n",
    "y_train_predict = lin_model.predict(X_train)\n",
    "r2 = metrics.r2_score(Y_train, y_train_predict)\n",
    "print('R2 Value (Train):',metrics.r2_score(Y_train, y_train_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794a60e7-0993-4079-9198-649787b41d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = (np.sqrt(mean_squared_error(Y_train, y_train_predict)))\n",
    "print(\"RMSE is (Train): {}\". format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db835cf3-922b-4c85-af8d-10ab71f6353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eavluating testing set\n",
    "y_test_predict = lin_model.predict(X_test)\n",
    "r2 = metrics.r2_score(Y_test, y_test_predict)\n",
    "print('Linear Regression R2 Value (Test):',metrics.r2_score(Y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8375e71d-0aab-40ef-b3c5-2f0b0f13c72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = (np.sqrt(mean_squared_error(Y_test, y_test_predict)))\n",
    "print('Linear Regression RMSE (Test) is {}'.format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074ad8c6-eb6f-4727-8a84-151b3f58d6a5",
   "metadata": {},
   "source": [
    "**Decision Tree Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7206c3a-e14d-4652-b3c1-ec6b8b707983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we take ['LSTAT','RM'] as independent variables while 'MEDV' as dependent variable for our models\n",
    "X = pd.DataFrame(np.c_[dataset['LSTAT'], dataset['RM']], columns = ['LSTAT','RM'])\n",
    "Y = dataset['MEDV']\n",
    "\n",
    "# Train size 70%\n",
    "# Test size 30%\n",
    "test_size = 0.30\n",
    "\n",
    "# Random state is 42\n",
    "seed = 42\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bf9b93-209c-4cf8-9f2f-02455fb056d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = DecisionTreeRegressor(max_depth=6)\n",
    "DT_reg = regressor.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d5cb25-f05c-4a04-bca5-3d998bedc7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Decision Tree Regression Score (Train):', DT_reg.score(X_train,Y_train))\n",
    "print('Decision Tree Regression Score (Test):', DT_reg.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bd61d1-f88b-4357-a7b7-6ae6161fda50",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure(figsize=(25,10))\n",
    "a = plot_tree(regressor, \n",
    "              feature_names=X.columns, \n",
    "              class_names=Y, \n",
    "              filled=True, \n",
    "              rounded=True, \n",
    "              fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ca2d40-4bbc-4ea1-9786-aaee66c2f633",
   "metadata": {},
   "source": [
    "**KNN Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3776a7-1c57-4021-bd20-e4c3d66c385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we take ['LSTAT','RM'] as independent variables while 'MEDV' as dependent variable for our models\n",
    "X = pd.DataFrame(np.c_[dataset['LSTAT'], dataset['RM']], columns = ['LSTAT','RM'])\n",
    "Y = dataset['MEDV']\n",
    "\n",
    "# Train size 70%\n",
    "# Test size 30%\n",
    "test_size = 0.30\n",
    "\n",
    "# Random state is 42\n",
    "seed = 42\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230f0be3-d921-4208-ba7a-24845ad38220",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_X = StandardScaler()\n",
    "\n",
    "X_train = ss_X.fit_transform(X_train)\n",
    "X_test = ss_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b3d032-1088-4e40-83e8-afba3c29e13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ss_Y = StandardScaler()\n",
    "#Y_train = ss_Y.fit_transform(Y_train.reshape(-1, 1))\n",
    "#Y_test = ss_Y.transform(Y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ae2eef-628e-452e-9c63-2e6a0dbcea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_knr = KNeighborsRegressor(weights=\"uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee52531-ac59-4c6e-aece-3f501197dbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_knr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eb0685-88e0-4beb-b31b-4ff026f770e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_knr_Y_predict = uni_knr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4568b90-eb73-4810-82d3-e7ea9b432633",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The average K-Nearest Neighbor Regression value:\", uni_knr.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31234975-73c7-40b9-ac7d-b18354536e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"K-Nearest Neighbor Regression R_squared Value:\" ,metrics.r2_score(Y_test, uni_knr_Y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a49a44c-95f0-48a6-a559-c3c74ee18e52",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In truth, machine learning is the cold hard maths of statistical analysis. But it is operated on a scale beyond regular human capabilities*. The scikit-learn library takes the wealth of knowledge available in those fields – from initial development to innovative use by its community – and presents an accessible toolkit for users. The distinction of the library as a toolkit is essential. The models and algorithms of machine learning will do as programmed. As a programmer, it is vital to understand the data, the models and algorithms available and the desired outcome to choose the best fit for the data you are working with.\n",
    "\n",
    "Scikit-learn makes it easy to explore the different modules it operates and build on models to improve their accuracy continually. It is easy to see why it is one of the most popular Python libraries.\n",
    "\n",
    "<br>\n",
    "\n",
    " \\* *Perhaps a new Marvel superhero called Numero who uses advanced statistical skills (following an incident with a calculator and a radioactive substance) to predict and prevent crimes.*\n",
    " \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb143d3-5ca3-4d50-b75c-29b185a3791b",
   "metadata": {},
   "source": [
    "# Bibliography\n",
    "\n",
    "1.\thttps://www.geeksforgeeks.org/introduction-of-statistics-and-its-types/?ref=lbp\n",
    "2.\thttps://jakevdp.github.io/PythonDataScienceHandbook/05.01-what-is-machine-learning.html\n",
    "3.\thttps://archive.ics.uci.edu/ml/datasets/wine\n",
    "4.\thttps://scikit-learn.org/stable/index.html\n",
    "5.\thttps://www.python.org/\n",
    "6.\t(https://summerofcode.withgoogle.com/\n",
    "7.\thttps://www.tutorialspoint.com/scikit_lear/scikit_learn_introduction.htm\n",
    "8.\thttps://scikit-learn.org/stable/model_selection.html,\n",
    "9.\thttps://scikit-learn.org/stable/data_transforms.html\n",
    "10.\thttps://www.zdnet.com/article/programming-languages-javascript-has-most-developers-but-rust-is-the-fastest-growing/\n",
    "11.\thttps://scipy.org/\n",
    "12.\thttps://steelkiwi.com/blog/python-for-ai-and-machine-learning/\n",
    "13.\thttps://towardsdatascience.com/clearly-explained-4-types-of-machine-learning-algorithms-71304380c59a \n",
    "14.\thttps://scikit-learn.org/stable/supervised_learning.html\n",
    "15.\thttps://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html?highlight=linear%20regression#sklearn.linear_model.LinearRegression\n",
    "16.\thttps://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "17.\thttps://towardsdatascience.com/clearly-explained-4-types-of-machine-learning-algorithms-71304380c59a \n",
    "18.\thttps://scikit-learn.org/stable/unsupervised_learning.html\n",
    "19.\thttps://scikit-learn.org/stable/modules/decomposition.html#factor-analysis\n",
    "20.\thttps://scikit-learn.org/stable/modules/outlier_detection.html#id1\n",
    "21.\thttps://towardsdatascience.com/clearly-explained-4-types-of-machine-learning-algorithms-71304380c59a\n",
    "22.\thttps://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.SelfTrainingClassifier.html\n",
    "23.\thttps://towardsdatascience.com/self-training-classifier-how-to-make-any-algorithm-behave-like-a-semi-supervised-one-2958e7b54ab7\n",
    "24.\thttps://towardsdatascience.com/clearly-explained-4-types-of-machine-learning-algorithms-71304380c59a\n",
    "25.\thttps://scikit-learn.org/stable/faq.html#why-is-there-no-support-for-deep-or-reinforcement-learning-will-there-be-support-for-deep-or-reinforcement-learning-in-scikit-learn\n",
    "26.\thttp://lib.stat.cmu.edu/datasets/boston\n",
    "27.\thttps://archive.ics.uci.edu/ml/index.php\n",
    "28.\thttps://towardsdatascience.com/things-you-didnt-know-about-the-boston-housing-dataset-2e87a6f960e8\n",
    "29.\thttps://scikit-learn.org/stable/inspection.html\n",
    "30.\thttps://towardsdatascience.com/8-commonly-used-pandas-display-options-you-should-know-a832365efa95\n",
    "31.\thttps://towardsdatascience.com/pearson-coefficient-of-correlation-explained-369991d93404\n",
    "32.\thttps://scikit-learn.org/stable/visualizations.html\n",
    "33.\thttps://www.w3schools.com/python/numpy/numpy_random_exponential.asp\n",
    "34.\thttps://towardsdatascience.com/histograms-and-density-plots-in-python-f6bda88f5ac0\n",
    "35.\thttps://www.w3schools.com/statistics/statistics_box_plots.php\n",
    "36.\thttps://towardsdatascience.com/better-heatmaps-and-correlation-matrix-plots-in-python-41445d0f2bec\n",
    "37.\thttps://www.geeksforgeeks.org/create-a-correlation-matrix-using-python/\n",
    "38.\thttps://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "39.\thttps://stackoverflow.com/questions/13610074/is-there-a-rule-of-thumb-for-how-to-divide-a-dataset-into-training-and-validatio\n",
    "40.\thttps://vitalflux.com/machine-learning-training-validation-test-data-set/\n",
    "41.\thttps://scikit-learn.org/stable/modules/cross_validation.html\n",
    "42.\thttps://towardsdatascience.com/k-fold-cross-validation-explained-in-plain-english-659e33c0bc0\n",
    "43.\thttps://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.RandomState.seed.htm\n",
    "44.\thttps://hitchhikers.fandom.com/wiki/42\n",
    "45.\thttps://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "46.\thttps://towardsdatascience.com/a-simple-guide-to-linear-regression-using-python-7050e8c751c1\n",
    "47.\thttps://www.analyticsvidhya.com/blog/2021/05/5-regression-algorithms-you-should-know-introductory-guide/\n",
    "48.\thttps://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "49.\thttps://towardsdatascience.com/building-a-k-nearest-neighbors-k-nn-model-with-scikit-learn-51209555453a\n",
    "50.\thttps://www.analyticsvidhya.com/blog/2018/08/k-nearest-neighbor-introduction-regression-python/ \n",
    "51.\thttps://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn\n",
    "52.\thttps://www.geeksforgeeks.org/python-decision-tree-regression-using-sklearn/\n",
    "53.\thttps://www.javatpoint.com/machine-learning-decision-tree-classification-algorithm\n",
    "54.\thttps://towardsdatascience.com/machine-learning-basics-decision-tree-regression-1d73ea003fda\n",
    "55.\thttps://towardsdatascience.com/how-and-why-to-standardize-your-data-996926c2c832\n",
    "56.\thttps://towardsdatascience.com/data-preprocessing-with-scikit-learn-standardization-and-scaling-cfb695280412\n",
    "57.\thttps://scikit-learn.org/stable/data_transforms.html\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "# References\n",
    "1.\thttps://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html\n",
    "2.\thttp://lib.stat.cmu.edu/datasets/boston"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d458084-9fdc-419b-a997-2bb9cfb7c65a",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
